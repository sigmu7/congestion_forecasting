{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-p71RPELW1ta"
   },
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "\n",
    "#import seaborn as sns\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 124
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 26163,
     "status": "ok",
     "timestamp": 1576335915756,
     "user": {
      "displayName": "Sandeep Mudigonda",
      "photoUrl": "",
      "userId": "02295463398705014066"
     },
     "user_tz": 300
    },
    "id": "EJh2SxxwW1tj",
    "outputId": "5143682d-a4e1-4117-ffbd-0843b25f35c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/gdrive\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')\n",
    "\n",
    "filename = 'df_highway_2012_4mon_sample.h5'\n",
    "\n",
    "df_LA = pd.read_hdf('/content/gdrive/My Drive/NNDL-data/' + filename)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2388,
     "status": "ok",
     "timestamp": 1575649229653,
     "user": {
      "displayName": "Sandeep Mudigonda",
      "photoUrl": "",
      "userId": "02295463398705014066"
     },
     "user_tz": 300
    },
    "id": "Dqz2dXmIW1tu",
    "outputId": "09446baa-c882-4142-f56a-2e25f4534e86"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(34272, 207)"
      ]
     },
     "execution_count": 3,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fdr_tt['measurement_tstamp'] = pd.to_datetime(fdr_tt['measurement_tstamp'])\n",
    "df_LA.dtypes\n",
    "df_LA.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0WrkzrFJFTJy"
   },
   "source": [
    "## Data Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 625,
     "status": "ok",
     "timestamp": 1576335923747,
     "user": {
      "displayName": "Sandeep Mudigonda",
      "photoUrl": "",
      "userId": "02295463398705014066"
     },
     "user_tz": 300
    },
    "id": "uJpH9jq6W1vl",
    "outputId": "dbfe43a6-f286-4ff4-a76d-86448eff3ff2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(34272, 207)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "spd_mat = df_LA.to_numpy()\n",
    "\n",
    "print(spd_mat.shape)\n",
    "\n",
    "spd_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "spd_mat = spd_scaler.fit_transform(spd_mat)\n",
    "\n",
    "#spd_std_scaler = StandardScaler(with_mean=True, with_std=True)\n",
    "#spd_mat = spd_std_scaler.fit_transform(spd_mat)\n",
    "\n",
    "n_out = spd_mat.shape[-1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0Kjr--ZTZ3mk"
   },
   "source": [
    "## Data Generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jZBcJFSvW1vt"
   },
   "outputs": [],
   "source": [
    "\n",
    "# This generator function is modified from Chollet (2018), Deep Learning with Python\n",
    "\n",
    "def generator_mat(data, lag, ahead, min_ind, max_ind, shuffle=False, batch_size=128, step=1):\n",
    "    if max_ind is None:\n",
    "        max_ind = len(data) - ahead - 1\n",
    "    i = min_ind + lag\n",
    "    while 1:\n",
    "        if shuffle:\n",
    "            rows = np.random.randint(min_ind+lag, max_ind, size=batch_size)\n",
    "        else:\n",
    "            if i+batch_size >= max_ind:\n",
    "                i=min_ind+lag\n",
    "            rows = np.arange(i,min(i+batch_size, max_ind))\n",
    "            i += len(rows)\n",
    "        \n",
    "        # change to following for multivar (>1 columns)\n",
    "        samples = np.zeros((len(rows),lag//step, data.shape[-1]))\n",
    "        targets = np.zeros((len(rows),data.shape[-1]))\n",
    "        \n",
    "        for j, row in enumerate(rows):\n",
    "            indices = range(rows[j]-lag, rows[j], step)\n",
    "            samples[j]= data[indices]\n",
    "            targets[j] = data[rows[j] + ahead][:] #index of output vectors\n",
    "            \n",
    "        yield samples, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UrEX2U6pW1wB"
   },
   "outputs": [],
   "source": [
    "# Setting data generation parameters\n",
    "\n",
    "lag = 3 # lag of 15 minutes\n",
    "step = 1 # this is a potential parameter to be optimized\n",
    "ahead = 3 # 3, 6, 12 to be used - predicting 15, 30, 60-min into the future\n",
    "batch_size = 128\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "esV9QhKIW1u6",
    "outputId": "2c2fa806-cb53-4978-bc51-09e3c8d643fe"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30528"
      ]
     },
     "execution_count": 73,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 119 days total\n",
    "# 70% -> 83 days for training = 23904 obs\n",
    "# 20% -> 23 days for valid\n",
    "# Rest -> test\n",
    "23*24*12 + 23904"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ANRlfrQMW1wK"
   },
   "outputs": [],
   "source": [
    "gen_tr = generator_mat(spd_mat,lag=lag,ahead=ahead,\n",
    "                   min_ind=0,max_ind=23904,shuffle=False,step=step,batch_size=batch_size)\n",
    "gen_val = generator_mat(spd_mat,lag=lag,ahead=ahead,\n",
    "                   min_ind=23905,max_ind=30528,step=step,batch_size=batch_size)\n",
    "gen_tes = generator_mat(spd_mat,lag=lag,ahead=ahead,\n",
    "                   min_ind=30529,max_ind=None,step=step,batch_size=batch_size)\n",
    "val_steps = (30528 - 23905 - lag) // batch_size\n",
    "test_steps = (34272 - 30529 - lag) // batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k4SGwtJIPyTZ"
   },
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "        return K.sqrt(K.mean(K.square(y_pred - y_true)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rwda0VVKZzuX"
   },
   "source": [
    "## Baseline Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 914,
     "status": "ok",
     "timestamp": 1576002041872,
     "user": {
      "displayName": "Sandeep Mudigonda",
      "photoUrl": "",
      "userId": "02295463398705014066"
     },
     "user_tz": 300
    },
    "id": "dXqrOG8bC0-U",
    "outputId": "9ee8f914-52dd-49c1-f93d-ffb79b3405e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.06417778790918481\n",
      "0.023816428493916134\n",
      "0.14381504134852688\n"
     ]
    }
   ],
   "source": [
    "# Simple baseline model\n",
    "def prev_week_val():\n",
    "    error_mae = []\n",
    "    error_mse = []\n",
    "    error_rmse = []\n",
    "    for step in range(val_steps):\n",
    "        samples, targets = next(gen_val)\n",
    "        preds = samples[:,-1,:]\n",
    "        # univariate\n",
    "        #preds = samples[:,-1]\n",
    "        mae = np.mean(np.abs(preds-targets))\n",
    "        mse = np.mean(np.square(preds-targets))\n",
    "        rmse = np.sqrt(mse)\n",
    "        error_mae.append(mae)\n",
    "        error_mse.append(mse)\n",
    "        error_rmse.append(rmse)\n",
    "    \n",
    "    #print(np.isnan(error_mape).sum)\n",
    "    print(np.mean(error_mae))\n",
    "    print(np.mean(error_mse))\n",
    "    print(np.mean(error_rmse))\n",
    "\n",
    "prev_week_val()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11165,
     "status": "ok",
     "timestamp": 1576002129521,
     "user": {
      "displayName": "Sandeep Mudigonda",
      "photoUrl": "",
      "userId": "02295463398705014066"
     },
     "user_tz": 300
    },
    "id": "n-H8M3FAGd2g",
    "outputId": "b3f6e953-73b3-4423-9029-4d06d77999d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAR\t1\t0.123\t0.044\t0.210\n",
      "VAR\t3\t0.123\t0.044\t0.210\n",
      "VAR\t6\t0.123\t0.044\t0.210\n",
      "VAR\t12\t0.123\t0.044\t0.210\n"
     ]
    }
   ],
   "source": [
    "# Baseline model for Vector Auto-Regression (VAR) time series model\n",
    "\n",
    "from statsmodels.tsa.vector_ar.var_model import VAR\n",
    "\n",
    "def eval_var(traffic_reading_df, n_lags=3):\n",
    "    n_forwards = [1, 3, 6, 12]\n",
    "    y_predicts, y_test = var_predict(traffic_reading_df, n_forwards = n_forwards,n_lags=n_lags,\n",
    "                                     test_ratio=0.3)\n",
    "    for i, horizon in enumerate(n_forwards):\n",
    "        mae = np.mean(np.abs(y_predicts-y_test))\n",
    "        mse =  np.mean(np.square(y_predicts-y_test))\n",
    "        rmse = np.sqrt(mse)\n",
    "        line = 'VAR\\t%d\\t%.3f\\t%.3f\\t%.3f' % (horizon, mae, mse, rmse)\n",
    "        print(line)\n",
    "        \n",
    "def var_predict(df, n_forwards=(1, 3), n_lags=4, test_ratio=0.2):\n",
    "    \"\"\"\n",
    "    Multivariate time series forecasting using Vector Auto-Regressive Model.\n",
    "    :param df: pandas.DataFrame, index: time, columns: sensor id, content: data.\n",
    "    :param n_forwards: a tuple of horizons.\n",
    "    :param n_lags: the order of the VAR model.\n",
    "    :param test_ratio:\n",
    "    :return: [list of prediction in different horizon], dt_test\n",
    "    \"\"\"\n",
    "    n_sample, n_output = df.shape\n",
    "    n_test = int(round(n_sample * test_ratio))\n",
    "    n_train = n_sample - n_test\n",
    "    df_train, df_test = df[:n_train], df[n_train:]\n",
    "\n",
    "    #scaler = StandardScaler(with_mean=True, with_std=True)\n",
    "    #data = scaler.fit_transform(df_train)\n",
    "    data = df_train\n",
    "    var_model = VAR(data)\n",
    "    var_result = var_model.fit(n_lags)\n",
    "    max_n_forwards = np.max(n_forwards)\n",
    "    \n",
    "    # Do forecasting.\n",
    "    result = np.zeros(shape=(len(n_forwards), n_test, n_output))\n",
    "    start = n_train - n_lags - max_n_forwards + 1\n",
    "    \n",
    "    for input_ind in range(start, n_sample - n_lags):\n",
    "#        prediction = var_result.forecast(scaler.transform(df[input_ind: input_ind + n_lags]), max_n_forwards)\n",
    "        prediction = var_result.forecast(df[input_ind: input_ind + n_lags], max_n_forwards)\n",
    "        for i, n_forward in enumerate(n_forwards):\n",
    "            result_ind = input_ind - n_train + n_lags + n_forward - 1\n",
    "            #print(result_ind)\n",
    "            if 0 <= result_ind < n_test:\n",
    "                result[i, result_ind, :] = prediction[n_forward - 1, :]\n",
    "    \n",
    "    df_predicts = []\n",
    "    for i, n_forward in enumerate(n_forwards):\n",
    "#        df_predicts = scaler.inverse_transform(result[i])\n",
    "        df_predicts = result[i]\n",
    "    return df_predicts, df_test\n",
    "  \n",
    "eval_var(spd_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_h_T1-7rZu_z"
   },
   "source": [
    "## Simpler single-layer Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 159072,
     "status": "ok",
     "timestamp": 1576007532420,
     "user": {
      "displayName": "Sandeep Mudigonda",
      "photoUrl": "",
      "userId": "02295463398705014066"
     },
     "user_tz": 300
    },
    "id": "3gdvsKxpW1wR",
    "outputId": "01904c76-e3f4-4073-f4d1-0e00669aca9e",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 621)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                19904     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 207)               13455     \n",
      "=================================================================\n",
      "Total params: 35,471\n",
      "Trainable params: 35,471\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/40\n",
      "500/500 [==============================] - 4s 9ms/step - loss: 0.2081 - val_loss: 0.1992\n",
      "Epoch 2/40\n",
      "500/500 [==============================] - 4s 8ms/step - loss: 0.1702 - val_loss: 0.1874\n",
      "Epoch 3/40\n",
      "500/500 [==============================] - 4s 8ms/step - loss: 0.1659 - val_loss: 0.1794\n",
      "Epoch 4/40\n",
      "500/500 [==============================] - 4s 8ms/step - loss: 0.1620 - val_loss: 0.1759\n",
      "Epoch 5/40\n",
      "500/500 [==============================] - 4s 8ms/step - loss: 0.1598 - val_loss: 0.1780\n",
      "Epoch 6/40\n",
      "500/500 [==============================] - 4s 8ms/step - loss: 0.1580 - val_loss: 0.1748\n",
      "Epoch 7/40\n",
      "500/500 [==============================] - 4s 8ms/step - loss: 0.1577 - val_loss: 0.1740\n",
      "Epoch 8/40\n",
      "500/500 [==============================] - 4s 8ms/step - loss: 0.1575 - val_loss: 0.1741\n",
      "Epoch 9/40\n",
      "500/500 [==============================] - 4s 8ms/step - loss: 0.1553 - val_loss: 0.1729\n",
      "Epoch 10/40\n",
      "500/500 [==============================] - 4s 8ms/step - loss: 0.1563 - val_loss: 0.1730\n",
      "Epoch 11/40\n",
      "500/500 [==============================] - 4s 8ms/step - loss: 0.1572 - val_loss: 0.1748\n",
      "Epoch 12/40\n",
      "500/500 [==============================] - 4s 8ms/step - loss: 0.1532 - val_loss: 0.1721\n",
      "Epoch 13/40\n",
      "500/500 [==============================] - 4s 8ms/step - loss: 0.1565 - val_loss: 0.1722\n",
      "Epoch 14/40\n",
      "500/500 [==============================] - 4s 8ms/step - loss: 0.1562 - val_loss: 0.1756\n",
      "Epoch 15/40\n",
      "500/500 [==============================] - 4s 8ms/step - loss: 0.1536 - val_loss: 0.1778\n",
      "Epoch 16/40\n",
      "500/500 [==============================] - 4s 8ms/step - loss: 0.1562 - val_loss: 0.1762\n",
      "Epoch 17/40\n",
      "500/500 [==============================] - 4s 8ms/step - loss: 0.1564 - val_loss: 0.1739\n",
      "Epoch 18/40\n",
      "500/500 [==============================] - 4s 8ms/step - loss: 0.1538 - val_loss: 0.1732\n",
      "Epoch 19/40\n",
      "500/500 [==============================] - 4s 8ms/step - loss: 0.1557 - val_loss: 0.1725\n",
      "Epoch 20/40\n",
      "500/500 [==============================] - 4s 8ms/step - loss: 0.1556 - val_loss: 0.1731\n",
      "Epoch 21/40\n",
      "500/500 [==============================] - 4s 8ms/step - loss: 0.1548 - val_loss: 0.1727\n",
      "Epoch 22/40\n",
      "500/500 [==============================] - 4s 8ms/step - loss: 0.1549 - val_loss: 0.1735\n",
      "Epoch 23/40\n",
      "500/500 [==============================] - 4s 8ms/step - loss: 0.1551 - val_loss: 0.1728\n",
      "Epoch 24/40\n",
      "500/500 [==============================] - 4s 8ms/step - loss: 0.1560 - val_loss: 0.1759\n",
      "Epoch 25/40\n",
      "500/500 [==============================] - 4s 9ms/step - loss: 0.1535 - val_loss: 0.1722\n",
      "Epoch 26/40\n",
      "500/500 [==============================] - 4s 8ms/step - loss: 0.1559 - val_loss: 0.1735\n",
      "Epoch 27/40\n",
      "500/500 [==============================] - 4s 8ms/step - loss: 0.1558 - val_loss: 0.1738\n",
      "Epoch 28/40\n",
      "500/500 [==============================] - 4s 9ms/step - loss: 0.1523 - val_loss: 0.1728\n",
      "Epoch 29/40\n",
      "500/500 [==============================] - 4s 8ms/step - loss: 0.1556 - val_loss: 0.1726\n",
      "Epoch 30/40\n",
      "500/500 [==============================] - 4s 8ms/step - loss: 0.1555 - val_loss: 0.1732\n",
      "Epoch 31/40\n",
      "500/500 [==============================] - 4s 8ms/step - loss: 0.1531 - val_loss: 0.1733\n",
      "Epoch 32/40\n",
      "500/500 [==============================] - 4s 8ms/step - loss: 0.1555 - val_loss: 0.1727\n",
      "Epoch 33/40\n",
      "500/500 [==============================] - 4s 8ms/step - loss: 0.1551 - val_loss: 0.1740\n",
      "Epoch 34/40\n",
      "500/500 [==============================] - 4s 8ms/step - loss: 0.1535 - val_loss: 0.1729\n",
      "Epoch 35/40\n",
      "500/500 [==============================] - 4s 8ms/step - loss: 0.1546 - val_loss: 0.1722\n",
      "Epoch 36/40\n",
      "500/500 [==============================] - 4s 8ms/step - loss: 0.1546 - val_loss: 0.1729\n",
      "Epoch 37/40\n",
      "500/500 [==============================] - 4s 8ms/step - loss: 0.1538 - val_loss: 0.1734\n",
      "Epoch 38/40\n",
      "500/500 [==============================] - 4s 8ms/step - loss: 0.1541 - val_loss: 0.1721\n",
      "Epoch 39/40\n",
      "500/500 [==============================] - 4s 8ms/step - loss: 0.1542 - val_loss: 0.1737\n",
      "Epoch 40/40\n",
      "500/500 [==============================] - 4s 8ms/step - loss: 0.1552 - val_loss: 0.1732\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(layers.Flatten(input_shape=(lag // step, spd_mat.shape[-1])))\n",
    "model.add(layers.Dense(32, activation='relu'))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(n_out))\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer=RMSprop(decay=0.001), loss=root_mean_squared_error)\n",
    "history = model.fit_generator(gen_tr,\n",
    "                              steps_per_epoch=500,\n",
    "                              epochs=40,\n",
    "                              validation_data=gen_val,\n",
    "                              validation_steps=val_steps)\n",
    "model.save('/content/gdrive/My Drive/NNDL-project/fnn_2_layer_64_decay_Std_3step_rmse.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 109388,
     "status": "ok",
     "timestamp": 1576005784313,
     "user": {
      "displayName": "Sandeep Mudigonda",
      "photoUrl": "",
      "userId": "02295463398705014066"
     },
     "user_tz": 300
    },
    "id": "iKp0K9qUQcAe",
    "outputId": "1e20b5fe-83f6-45ba-adb7-222ff4c8fe65"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_3 (Flatten)          (None, 621)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 32)                19904     \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 207)               13455     \n",
      "=================================================================\n",
      "Total params: 35,471\n",
      "Trainable params: 35,471\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/40\n",
      "500/500 [==============================] - 3s 6ms/step - loss: 0.2111 - val_loss: 0.2088\n",
      "Epoch 2/40\n",
      "500/500 [==============================] - 3s 5ms/step - loss: 0.1724 - val_loss: 0.1886\n",
      "Epoch 3/40\n",
      "500/500 [==============================] - 3s 5ms/step - loss: 0.1676 - val_loss: 0.1816\n",
      "Epoch 4/40\n",
      "500/500 [==============================] - 3s 5ms/step - loss: 0.1636 - val_loss: 0.1768\n",
      "Epoch 5/40\n",
      "500/500 [==============================] - 3s 5ms/step - loss: 0.1617 - val_loss: 0.1846\n",
      "Epoch 6/40\n",
      "500/500 [==============================] - 3s 5ms/step - loss: 0.1596 - val_loss: 0.1773\n",
      "Epoch 7/40\n",
      "500/500 [==============================] - 3s 5ms/step - loss: 0.1584 - val_loss: 0.1778\n",
      "Epoch 8/40\n",
      "500/500 [==============================] - 3s 5ms/step - loss: 0.1580 - val_loss: 0.1794\n",
      "Epoch 9/40\n",
      "500/500 [==============================] - 3s 5ms/step - loss: 0.1552 - val_loss: 0.1742\n",
      "Epoch 10/40\n",
      "500/500 [==============================] - 3s 5ms/step - loss: 0.1552 - val_loss: 0.1744\n",
      "Epoch 11/40\n",
      "500/500 [==============================] - 3s 5ms/step - loss: 0.1557 - val_loss: 0.1926\n",
      "Epoch 12/40\n",
      "500/500 [==============================] - 3s 5ms/step - loss: 0.1523 - val_loss: 0.1738\n",
      "Epoch 13/40\n",
      "500/500 [==============================] - 3s 5ms/step - loss: 0.1543 - val_loss: 0.1711\n",
      "Epoch 14/40\n",
      "500/500 [==============================] - 3s 5ms/step - loss: 0.1536 - val_loss: 0.1766\n",
      "Epoch 15/40\n",
      "500/500 [==============================] - 3s 5ms/step - loss: 0.1513 - val_loss: 0.1784\n",
      "Epoch 16/40\n",
      "500/500 [==============================] - 3s 5ms/step - loss: 0.1531 - val_loss: 0.1745\n",
      "Epoch 17/40\n",
      "500/500 [==============================] - 3s 5ms/step - loss: 0.1529 - val_loss: 0.1723\n",
      "Epoch 18/40\n",
      "500/500 [==============================] - 3s 5ms/step - loss: 0.1505 - val_loss: 0.1806\n",
      "Epoch 19/40\n",
      "500/500 [==============================] - 3s 5ms/step - loss: 0.1520 - val_loss: 0.1706\n",
      "Epoch 20/40\n",
      "500/500 [==============================] - 3s 5ms/step - loss: 0.1515 - val_loss: 0.1707\n",
      "Epoch 21/40\n",
      "500/500 [==============================] - 3s 5ms/step - loss: 0.1508 - val_loss: 0.1729\n",
      "Epoch 22/40\n",
      "500/500 [==============================] - 3s 5ms/step - loss: 0.1513 - val_loss: 0.1706\n",
      "Epoch 23/40\n",
      "500/500 [==============================] - 3s 5ms/step - loss: 0.1507 - val_loss: 0.1699\n",
      "Epoch 24/40\n",
      "500/500 [==============================] - 3s 6ms/step - loss: 0.1518 - val_loss: 0.1729\n",
      "Epoch 25/40\n",
      "500/500 [==============================] - 3s 5ms/step - loss: 0.1498 - val_loss: 0.1691\n",
      "Epoch 26/40\n",
      "500/500 [==============================] - 3s 5ms/step - loss: 0.1514 - val_loss: 0.1692\n",
      "Epoch 27/40\n",
      "500/500 [==============================] - 3s 5ms/step - loss: 0.1515 - val_loss: 0.1735\n",
      "Epoch 28/40\n",
      "500/500 [==============================] - 3s 5ms/step - loss: 0.1488 - val_loss: 0.1689\n",
      "Epoch 29/40\n",
      "500/500 [==============================] - 3s 5ms/step - loss: 0.1514 - val_loss: 0.1691\n",
      "Epoch 30/40\n",
      "500/500 [==============================] - 3s 5ms/step - loss: 0.1514 - val_loss: 0.1712\n",
      "Epoch 31/40\n",
      "500/500 [==============================] - 3s 5ms/step - loss: 0.1497 - val_loss: 0.1704\n",
      "Epoch 32/40\n",
      "500/500 [==============================] - 3s 6ms/step - loss: 0.1518 - val_loss: 0.1693\n",
      "Epoch 33/40\n",
      "500/500 [==============================] - 3s 5ms/step - loss: 0.1514 - val_loss: 0.1706\n",
      "Epoch 34/40\n",
      "500/500 [==============================] - 3s 5ms/step - loss: 0.1504 - val_loss: 0.1708\n",
      "Epoch 35/40\n",
      "500/500 [==============================] - 3s 5ms/step - loss: 0.1517 - val_loss: 0.1692\n",
      "Epoch 36/40\n",
      "500/500 [==============================] - 3s 5ms/step - loss: 0.1515 - val_loss: 0.1699\n",
      "Epoch 37/40\n",
      "500/500 [==============================] - 3s 6ms/step - loss: 0.1510 - val_loss: 0.1699\n",
      "Epoch 38/40\n",
      "500/500 [==============================] - 3s 5ms/step - loss: 0.1518 - val_loss: 0.1695\n",
      "Epoch 39/40\n",
      "500/500 [==============================] - 3s 6ms/step - loss: 0.1516 - val_loss: 0.1704\n",
      "Epoch 40/40\n",
      "500/500 [==============================] - 3s 6ms/step - loss: 0.1528 - val_loss: 0.1709\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(layers.Flatten(input_shape=(lag // step, spd_mat.shape[-1])))\n",
    "model.add(layers.Dense(32, activation='relu'))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(n_out))\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer=RMSprop(decay=0.001), loss=root_mean_squared_error)\n",
    "history = model.fit_generator(gen_tr,\n",
    "                              steps_per_epoch=500,\n",
    "                              epochs=40,\n",
    "                              validation_data=gen_val,\n",
    "                              validation_steps=val_steps)\n",
    "model.save('/content/gdrive/My Drive/NNDL-project/fnn_2_layer_64_Std_3step_rmse.h5')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 213560,
     "status": "ok",
     "timestamp": 1576006034473,
     "user": {
      "displayName": "Sandeep Mudigonda",
      "photoUrl": "",
      "userId": "02295463398705014066"
     },
     "user_tz": 300
    },
    "id": "P7Y1vQoi-aAX",
    "outputId": "44686708-3cee-43c8-fa08-82eeff533797"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_1 (GRU)                  (None, 64)                52224     \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 207)               13455     \n",
      "=================================================================\n",
      "Total params: 65,679\n",
      "Trainable params: 65,679\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/40\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 0.1865 - val_loss: 0.1865\n",
      "Epoch 2/40\n",
      "500/500 [==============================] - 5s 11ms/step - loss: 0.1665 - val_loss: 0.1896\n",
      "Epoch 3/40\n",
      "500/500 [==============================] - 5s 11ms/step - loss: 0.1615 - val_loss: 0.1777\n",
      "Epoch 4/40\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 0.1557 - val_loss: 0.1946\n",
      "Epoch 5/40\n",
      "500/500 [==============================] - 5s 11ms/step - loss: 0.1541 - val_loss: 0.1806\n",
      "Epoch 6/40\n",
      "500/500 [==============================] - 5s 11ms/step - loss: 0.1508 - val_loss: 0.1753\n",
      "Epoch 7/40\n",
      "500/500 [==============================] - 5s 11ms/step - loss: 0.1482 - val_loss: 0.2033\n",
      "Epoch 8/40\n",
      "500/500 [==============================] - 5s 10ms/step - loss: 0.1476 - val_loss: 0.1732\n",
      "Epoch 9/40\n",
      "500/500 [==============================] - 5s 11ms/step - loss: 0.1453 - val_loss: 0.1668\n",
      "Epoch 10/40\n",
      "500/500 [==============================] - 5s 11ms/step - loss: 0.1435 - val_loss: 0.1685\n",
      "Epoch 11/40\n",
      "500/500 [==============================] - 5s 10ms/step - loss: 0.1437 - val_loss: 0.1681\n",
      "Epoch 12/40\n",
      "500/500 [==============================] - 5s 11ms/step - loss: 0.1410 - val_loss: 0.1682\n",
      "Epoch 13/40\n",
      "500/500 [==============================] - 5s 11ms/step - loss: 0.1418 - val_loss: 0.1910\n",
      "Epoch 14/40\n",
      "500/500 [==============================] - 5s 10ms/step - loss: 0.1395 - val_loss: 0.1798\n",
      "Epoch 15/40\n",
      "500/500 [==============================] - 5s 10ms/step - loss: 0.1391 - val_loss: 0.1676\n",
      "Epoch 16/40\n",
      "500/500 [==============================] - 5s 11ms/step - loss: 0.1385 - val_loss: 0.1689\n",
      "Epoch 17/40\n",
      "500/500 [==============================] - 5s 11ms/step - loss: 0.1369 - val_loss: 0.1642\n",
      "Epoch 18/40\n",
      "500/500 [==============================] - 5s 10ms/step - loss: 0.1367 - val_loss: 0.1647\n",
      "Epoch 19/40\n",
      "500/500 [==============================] - 5s 10ms/step - loss: 0.1365 - val_loss: 0.1603\n",
      "Epoch 20/40\n",
      "500/500 [==============================] - 5s 11ms/step - loss: 0.1355 - val_loss: 0.1689\n",
      "Epoch 21/40\n",
      "500/500 [==============================] - 5s 10ms/step - loss: 0.1357 - val_loss: 0.1651\n",
      "Epoch 22/40\n",
      "500/500 [==============================] - 5s 11ms/step - loss: 0.1343 - val_loss: 0.1686\n",
      "Epoch 23/40\n",
      "500/500 [==============================] - 5s 10ms/step - loss: 0.1333 - val_loss: 0.1577\n",
      "Epoch 24/40\n",
      "500/500 [==============================] - 5s 10ms/step - loss: 0.1342 - val_loss: 0.1581\n",
      "Epoch 25/40\n",
      "500/500 [==============================] - 5s 10ms/step - loss: 0.1325 - val_loss: 0.1646\n",
      "Epoch 26/40\n",
      "500/500 [==============================] - 5s 10ms/step - loss: 0.1320 - val_loss: 0.1638\n",
      "Epoch 27/40\n",
      "500/500 [==============================] - 5s 10ms/step - loss: 0.1329 - val_loss: 0.1638\n",
      "Epoch 28/40\n",
      "500/500 [==============================] - 5s 11ms/step - loss: 0.1307 - val_loss: 0.1686\n",
      "Epoch 29/40\n",
      "500/500 [==============================] - 5s 11ms/step - loss: 0.1322 - val_loss: 0.1862\n",
      "Epoch 30/40\n",
      "500/500 [==============================] - 5s 11ms/step - loss: 0.1306 - val_loss: 0.1648\n",
      "Epoch 31/40\n",
      "500/500 [==============================] - 5s 11ms/step - loss: 0.1306 - val_loss: 0.1618\n",
      "Epoch 32/40\n",
      "500/500 [==============================] - 5s 10ms/step - loss: 0.1305 - val_loss: 0.1597\n",
      "Epoch 33/40\n",
      "500/500 [==============================] - 5s 11ms/step - loss: 0.1295 - val_loss: 0.1614\n",
      "Epoch 34/40\n",
      "500/500 [==============================] - 5s 11ms/step - loss: 0.1296 - val_loss: 0.1590\n",
      "Epoch 35/40\n",
      "500/500 [==============================] - 5s 10ms/step - loss: 0.1297 - val_loss: 0.1637\n",
      "Epoch 36/40\n",
      "500/500 [==============================] - 5s 11ms/step - loss: 0.1288 - val_loss: 0.1702\n",
      "Epoch 37/40\n",
      "500/500 [==============================] - 5s 11ms/step - loss: 0.1293 - val_loss: 0.1624\n",
      "Epoch 38/40\n",
      "500/500 [==============================] - 5s 10ms/step - loss: 0.1284 - val_loss: 0.2030\n",
      "Epoch 39/40\n",
      "500/500 [==============================] - 5s 10ms/step - loss: 0.1276 - val_loss: 0.1598\n",
      "Epoch 40/40\n",
      "500/500 [==============================] - 5s 10ms/step - loss: 0.1289 - val_loss: 0.1557\n"
     ]
    }
   ],
   "source": [
    "# Single layer RNN: GRU \n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.GRU(64,\n",
    "                     input_shape=(None, spd_mat.shape[-1])))\n",
    "model.add(layers.Dense(n_out))\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer=RMSprop(), loss=root_mean_squared_error)\n",
    "history = model.fit_generator(gen_tr,\n",
    "                              steps_per_epoch=500,\n",
    "                              epochs=40,\n",
    "                              validation_data=gen_val,\n",
    "                              validation_steps=val_steps)\n",
    "model.save('/content/gdrive/My Drive/NNDL-project/gru_1_layer_simple_64_3step_rmse.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 225135,
     "status": "ok",
     "timestamp": 1576342930954,
     "user": {
      "displayName": "Sandeep Mudigonda",
      "photoUrl": "",
      "userId": "02295463398705014066"
     },
     "user_tz": 300
    },
    "id": "etcBQzRG1ITo",
    "outputId": "fcfb74a3-1e7c-4c56-b75b-a2132ed2363f",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_15 (GRU)                 (None, 128)               129024    \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 207)               26703     \n",
      "=================================================================\n",
      "Total params: 155,727\n",
      "Trainable params: 155,727\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/40\n",
      "500/500 [==============================] - 9s 17ms/step - loss: 0.1800 - val_loss: 0.1861\n",
      "Epoch 2/40\n",
      "500/500 [==============================] - 5s 11ms/step - loss: 0.1555 - val_loss: 0.1759\n",
      "Epoch 3/40\n",
      "500/500 [==============================] - 5s 11ms/step - loss: 0.1466 - val_loss: 0.1679\n",
      "Epoch 4/40\n",
      "500/500 [==============================] - 5s 11ms/step - loss: 0.1412 - val_loss: 0.1632\n",
      "Epoch 5/40\n",
      "500/500 [==============================] - 5s 11ms/step - loss: 0.1387 - val_loss: 0.1620\n",
      "Epoch 6/40\n",
      "500/500 [==============================] - 5s 11ms/step - loss: 0.1351 - val_loss: 0.1627\n",
      "Epoch 7/40\n",
      "500/500 [==============================] - 5s 11ms/step - loss: 0.1341 - val_loss: 0.1604\n",
      "Epoch 8/40\n",
      "500/500 [==============================] - 5s 11ms/step - loss: 0.1321 - val_loss: 0.1570\n",
      "Epoch 9/40\n",
      "500/500 [==============================] - 5s 11ms/step - loss: 0.1301 - val_loss: 0.1604\n",
      "Epoch 10/40\n",
      "500/500 [==============================] - 5s 11ms/step - loss: 0.1305 - val_loss: 0.1633\n",
      "Epoch 11/40\n",
      "500/500 [==============================] - 6s 11ms/step - loss: 0.1282 - val_loss: 0.1539\n",
      "Epoch 12/40\n",
      "500/500 [==============================] - 5s 11ms/step - loss: 0.1284 - val_loss: 0.1549\n",
      "Epoch 13/40\n",
      "500/500 [==============================] - 5s 11ms/step - loss: 0.1273 - val_loss: 0.1523\n",
      "Epoch 14/40\n",
      "500/500 [==============================] - 5s 11ms/step - loss: 0.1265 - val_loss: 0.1562\n",
      "Epoch 15/40\n",
      "500/500 [==============================] - 5s 11ms/step - loss: 0.1268 - val_loss: 0.1554\n",
      "Epoch 16/40\n",
      "500/500 [==============================] - 5s 11ms/step - loss: 0.1264 - val_loss: 0.1504\n",
      "Epoch 17/40\n",
      "500/500 [==============================] - 5s 11ms/step - loss: 0.1246 - val_loss: 0.1526\n",
      "Epoch 18/40\n",
      "500/500 [==============================] - 5s 11ms/step - loss: 0.1258 - val_loss: 0.1492\n",
      "Epoch 19/40\n",
      "500/500 [==============================] - 5s 11ms/step - loss: 0.1247 - val_loss: 0.1486\n",
      "Epoch 20/40\n",
      "500/500 [==============================] - 5s 11ms/step - loss: 0.1237 - val_loss: 0.1484\n",
      "Epoch 21/40\n",
      "500/500 [==============================] - 5s 11ms/step - loss: 0.1254 - val_loss: 0.1495\n",
      "Epoch 22/40\n",
      "500/500 [==============================] - 6s 11ms/step - loss: 0.1232 - val_loss: 0.1474\n",
      "Epoch 23/40\n",
      "500/500 [==============================] - 5s 11ms/step - loss: 0.1244 - val_loss: 0.1522\n",
      "Epoch 24/40\n",
      "500/500 [==============================] - 5s 11ms/step - loss: 0.1234 - val_loss: 0.1474\n",
      "Epoch 25/40\n",
      "500/500 [==============================] - 5s 11ms/step - loss: 0.1232 - val_loss: 0.1513\n",
      "Epoch 26/40\n",
      "500/500 [==============================] - 5s 11ms/step - loss: 0.1232 - val_loss: 0.1482\n",
      "Epoch 27/40\n",
      "500/500 [==============================] - 5s 11ms/step - loss: 0.1224 - val_loss: 0.1470\n",
      "Epoch 28/40\n",
      "500/500 [==============================] - 5s 11ms/step - loss: 0.1229 - val_loss: 0.1466\n",
      "Epoch 29/40\n",
      "500/500 [==============================] - 5s 11ms/step - loss: 0.1224 - val_loss: 0.1462\n",
      "Epoch 30/40\n",
      "500/500 [==============================] - 5s 11ms/step - loss: 0.1223 - val_loss: 0.1480\n",
      "Epoch 31/40\n",
      "500/500 [==============================] - 5s 11ms/step - loss: 0.1229 - val_loss: 0.1466\n",
      "Epoch 32/40\n",
      "500/500 [==============================] - 5s 11ms/step - loss: 0.1222 - val_loss: 0.1459\n",
      "Epoch 33/40\n",
      "500/500 [==============================] - 5s 11ms/step - loss: 0.1213 - val_loss: 0.1460\n",
      "Epoch 34/40\n",
      "500/500 [==============================] - 5s 11ms/step - loss: 0.1229 - val_loss: 0.1466\n",
      "Epoch 35/40\n",
      "500/500 [==============================] - 5s 11ms/step - loss: 0.1215 - val_loss: 0.1451\n",
      "Epoch 36/40\n",
      "500/500 [==============================] - 5s 11ms/step - loss: 0.1207 - val_loss: 0.1449\n",
      "Epoch 37/40\n",
      "500/500 [==============================] - 5s 11ms/step - loss: 0.1228 - val_loss: 0.1449\n",
      "Epoch 38/40\n",
      "500/500 [==============================] - 5s 11ms/step - loss: 0.1207 - val_loss: 0.1449\n",
      "Epoch 39/40\n",
      "500/500 [==============================] - 5s 11ms/step - loss: 0.1220 - val_loss: 0.1459\n",
      "Epoch 40/40\n",
      "500/500 [==============================] - 5s 11ms/step - loss: 0.1212 - val_loss: 0.1446\n"
     ]
    }
   ],
   "source": [
    "# Single layer RNN: GRU \n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.GRU(128,\n",
    "                     input_shape=(None, spd_mat.shape[-1])))\n",
    "model.add(layers.Dense(n_out))\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer=RMSprop(decay=0.001), loss=root_mean_squared_error)\n",
    "m2 = model\n",
    "history = m2.fit_generator(gen_tr,\n",
    "                              steps_per_epoch=500,\n",
    "                              epochs=40,\n",
    "                              validation_data=gen_val,\n",
    "                              validation_steps=val_steps)\n",
    "model.save('/content/gdrive/My Drive/NNDL-project/gru_1_layer_128_3step_rmse.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RDr-eTFZForP"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(len(loss))\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'r', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss for 1 Layer GRU 64 units w/ Dropout')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 236344,
     "status": "ok",
     "timestamp": 1576336577272,
     "user": {
      "displayName": "Sandeep Mudigonda",
      "photoUrl": "",
      "userId": "02295463398705014066"
     },
     "user_tz": 300
    },
    "id": "GgbWn_3Zb3zM",
    "outputId": "5ac56aea-956d-4c6f-c6f6-289a067c0fda",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 64)                69632     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 207)               13455     \n",
      "=================================================================\n",
      "Total params: 83,087\n",
      "Trainable params: 83,087\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/40\n",
      "500/500 [==============================] - 7s 13ms/step - loss: 0.1877 - val_loss: 0.1818\n",
      "Epoch 2/40\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 0.1605 - val_loss: 0.1803\n",
      "Epoch 3/40\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 0.1564 - val_loss: 0.1720\n",
      "Epoch 4/40\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 0.1508 - val_loss: 0.1691\n",
      "Epoch 5/40\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 0.1475 - val_loss: 0.1703\n",
      "Epoch 6/40\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 0.1456 - val_loss: 0.1667\n",
      "Epoch 7/40\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 0.1431 - val_loss: 0.1657\n",
      "Epoch 8/40\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 0.1426 - val_loss: 0.1652\n",
      "Epoch 9/40\n",
      "500/500 [==============================] - 6s 11ms/step - loss: 0.1405 - val_loss: 0.1632\n",
      "Epoch 10/40\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 0.1390 - val_loss: 0.1636\n",
      "Epoch 11/40\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 0.1393 - val_loss: 0.1658\n",
      "Epoch 12/40\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 0.1370 - val_loss: 0.1607\n",
      "Epoch 13/40\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 0.1372 - val_loss: 0.1616\n",
      "Epoch 14/40\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 0.1363 - val_loss: 0.1605\n",
      "Epoch 15/40\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 0.1354 - val_loss: 0.1619\n",
      "Epoch 16/40\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 0.1355 - val_loss: 0.1596\n",
      "Epoch 17/40\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 0.1351 - val_loss: 0.1590\n",
      "Epoch 18/40\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 0.1336 - val_loss: 0.1612\n",
      "Epoch 19/40\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 0.1344 - val_loss: 0.1574\n",
      "Epoch 20/40\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 0.1333 - val_loss: 0.1568\n",
      "Epoch 21/40\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 0.1326 - val_loss: 0.1570\n",
      "Epoch 22/40\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 0.1337 - val_loss: 0.1569\n",
      "Epoch 23/40\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 0.1317 - val_loss: 0.1556\n",
      "Epoch 24/40\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 0.1332 - val_loss: 0.1586\n",
      "Epoch 25/40\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 0.1317 - val_loss: 0.1555\n",
      "Epoch 26/40\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 0.1316 - val_loss: 0.1582\n",
      "Epoch 27/40\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 0.1317 - val_loss: 0.1553\n",
      "Epoch 28/40\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 0.1306 - val_loss: 0.1546\n",
      "Epoch 29/40\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 0.1310 - val_loss: 0.1541\n",
      "Epoch 30/40\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 0.1308 - val_loss: 0.1543\n",
      "Epoch 31/40\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 0.1305 - val_loss: 0.1549\n",
      "Epoch 32/40\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 0.1309 - val_loss: 0.1539\n",
      "Epoch 33/40\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 0.1302 - val_loss: 0.1542\n",
      "Epoch 34/40\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 0.1295 - val_loss: 0.1540\n",
      "Epoch 35/40\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 0.1307 - val_loss: 0.1545\n",
      "Epoch 36/40\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 0.1294 - val_loss: 0.1528\n",
      "Epoch 37/40\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 0.1288 - val_loss: 0.1529\n",
      "Epoch 38/40\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 0.1305 - val_loss: 0.1528\n",
      "Epoch 39/40\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 0.1285 - val_loss: 0.1531\n",
      "Epoch 40/40\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 0.1300 - val_loss: 0.1526\n"
     ]
    }
   ],
   "source": [
    "# LSTM 64 units \n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.LSTM(64,\n",
    "                     input_shape=(None, spd_mat.shape[-1])))\n",
    "model.add(layers.Dense(n_out))\n",
    "\n",
    "model.summary()\n",
    "model.compile(optimizer=RMSprop(decay=0.001), loss=root_mean_squared_error)\n",
    "history = model.fit_generator(gen_tr,\n",
    "                              steps_per_epoch=500,\n",
    "                              epochs=40,\n",
    "                              validation_data=gen_val,\n",
    "                              validation_steps=val_steps)\n",
    "model.save('/content/gdrive/My Drive/NNDL-project/lstm_1_layer_64_Std_3step_rmse.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 251219,
     "status": "ok",
     "timestamp": 1576341465861,
     "user": {
      "displayName": "Sandeep Mudigonda",
      "photoUrl": "",
      "userId": "02295463398705014066"
     },
     "user_tz": 300
    },
    "id": "HaO9MyxHQcjd",
    "outputId": "7fae74b3-e92a-42de-c054-d5eb4d5e62aa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_4 (LSTM)                (None, 128)               172032    \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 207)               26703     \n",
      "=================================================================\n",
      "Total params: 198,735\n",
      "Trainable params: 198,735\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/40\n",
      "500/500 [==============================] - 9s 18ms/step - loss: 0.1798 - val_loss: 0.1897\n",
      "Epoch 2/40\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 0.1547 - val_loss: 0.1728\n",
      "Epoch 3/40\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 0.1470 - val_loss: 0.1683\n",
      "Epoch 4/40\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 0.1408 - val_loss: 0.1665\n",
      "Epoch 5/40\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 0.1389 - val_loss: 0.1877\n",
      "Epoch 6/40\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 0.1344 - val_loss: 0.1630\n",
      "Epoch 7/40\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 0.1334 - val_loss: 0.1574\n",
      "Epoch 8/40\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 0.1320 - val_loss: 0.1569\n",
      "Epoch 9/40\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 0.1297 - val_loss: 0.1577\n",
      "Epoch 10/40\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 0.1293 - val_loss: 0.1540\n",
      "Epoch 11/40\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 0.1290 - val_loss: 0.1529\n",
      "Epoch 12/40\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 0.1276 - val_loss: 0.1545\n",
      "Epoch 13/40\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 0.1277 - val_loss: 0.1512\n",
      "Epoch 14/40\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 0.1266 - val_loss: 0.1545\n",
      "Epoch 15/40\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 0.1254 - val_loss: 0.1503\n",
      "Epoch 16/40\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 0.1266 - val_loss: 0.1497\n",
      "Epoch 17/40\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 0.1247 - val_loss: 0.1490\n",
      "Epoch 18/40\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 0.1247 - val_loss: 0.1491\n",
      "Epoch 19/40\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 0.1253 - val_loss: 0.1497\n",
      "Epoch 20/40\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 0.1232 - val_loss: 0.1506\n",
      "Epoch 21/40\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 0.1250 - val_loss: 0.1493\n",
      "Epoch 22/40\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 0.1233 - val_loss: 0.1471\n",
      "Epoch 23/40\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 0.1236 - val_loss: 0.1471\n",
      "Epoch 24/40\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 0.1235 - val_loss: 0.1471\n",
      "Epoch 25/40\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 0.1224 - val_loss: 0.1479\n",
      "Epoch 26/40\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 0.1233 - val_loss: 0.1478\n",
      "Epoch 27/40\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 0.1231 - val_loss: 0.1493\n",
      "Epoch 28/40\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 0.1217 - val_loss: 0.1462\n",
      "Epoch 29/40\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 0.1231 - val_loss: 0.1465\n",
      "Epoch 30/40\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 0.1219 - val_loss: 0.1459\n",
      "Epoch 31/40\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 0.1212 - val_loss: 0.1460\n",
      "Epoch 32/40\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 0.1228 - val_loss: 0.1453\n",
      "Epoch 33/40\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 0.1213 - val_loss: 0.1453\n",
      "Epoch 34/40\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 0.1218 - val_loss: 0.1467\n",
      "Epoch 35/40\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 0.1220 - val_loss: 0.1456\n",
      "Epoch 36/40\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 0.1206 - val_loss: 0.1459\n",
      "Epoch 37/40\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 0.1220 - val_loss: 0.1464\n",
      "Epoch 38/40\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 0.1206 - val_loss: 0.1446\n",
      "Epoch 39/40\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 0.1212 - val_loss: 0.1456\n",
      "Epoch 40/40\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 0.1211 - val_loss: 0.1449\n"
     ]
    }
   ],
   "source": [
    "# LSTM 128 units \n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.LSTM(128,\n",
    "                     input_shape=(None, spd_mat.shape[-1])))\n",
    "model.add(layers.Dense(n_out))\n",
    "\n",
    "model.summary()\n",
    "model.compile(optimizer=RMSprop(decay=0.001), loss=root_mean_squared_error)\n",
    "history = model.fit_generator(gen_tr,\n",
    "                              steps_per_epoch=500,\n",
    "                              epochs=40,\n",
    "                              validation_data=gen_val,\n",
    "                              validation_steps=val_steps)\n",
    "model.save('/content/gdrive/My Drive/NNDL-project/lstm_1_layer_128_Std_3step_rmse.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qpKW6BXp6xEX"
   },
   "source": [
    "## Deeper Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 329848,
     "status": "ok",
     "timestamp": 1576337180912,
     "user": {
      "displayName": "Sandeep Mudigonda",
      "photoUrl": "",
      "userId": "02295463398705014066"
     },
     "user_tz": 300
    },
    "id": "HfritW2mRygT",
    "outputId": "af11491b-9d2c-4e8e-9509-311662f39375"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_2 (GRU)                  (None, None, 64)          52224     \n",
      "_________________________________________________________________\n",
      "gru_3 (GRU)                  (None, 64)                24768     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 207)               13455     \n",
      "=================================================================\n",
      "Total params: 90,447\n",
      "Trainable params: 90,447\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/40\n",
      "500/500 [==============================] - 9s 19ms/step - loss: 0.1907 - val_loss: 0.1836\n",
      "Epoch 2/40\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.1666 - val_loss: 0.1847\n",
      "Epoch 3/40\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.1612 - val_loss: 0.1770\n",
      "Epoch 4/40\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.1548 - val_loss: 0.1798\n",
      "Epoch 5/40\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.1537 - val_loss: 0.1715\n",
      "Epoch 6/40\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.1506 - val_loss: 0.1703\n",
      "Epoch 7/40\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.1480 - val_loss: 0.1760\n",
      "Epoch 8/40\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.1476 - val_loss: 0.1662\n",
      "Epoch 9/40\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.1460 - val_loss: 0.1656\n",
      "Epoch 10/40\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.1446 - val_loss: 0.1655\n",
      "Epoch 11/40\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.1446 - val_loss: 0.1646\n",
      "Epoch 12/40\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.1431 - val_loss: 0.1640\n",
      "Epoch 13/40\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.1441 - val_loss: 0.1686\n",
      "Epoch 14/40\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.1412 - val_loss: 0.1658\n",
      "Epoch 15/40\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.1423 - val_loss: 0.1629\n",
      "Epoch 16/40\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.1418 - val_loss: 0.1649\n",
      "Epoch 17/40\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.1394 - val_loss: 0.1624\n",
      "Epoch 18/40\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.1404 - val_loss: 0.1618\n",
      "Epoch 19/40\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.1406 - val_loss: 0.1623\n",
      "Epoch 20/40\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.1391 - val_loss: 0.1619\n",
      "Epoch 21/40\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.1400 - val_loss: 0.1615\n",
      "Epoch 22/40\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.1388 - val_loss: 0.1629\n",
      "Epoch 23/40\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.1383 - val_loss: 0.1614\n",
      "Epoch 24/40\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.1386 - val_loss: 0.1606\n",
      "Epoch 25/40\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.1379 - val_loss: 0.1604\n",
      "Epoch 26/40\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.1378 - val_loss: 0.1602\n",
      "Epoch 27/40\n",
      "500/500 [==============================] - 8s 17ms/step - loss: 0.1378 - val_loss: 0.1603\n",
      "Epoch 28/40\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.1369 - val_loss: 0.1606\n",
      "Epoch 29/40\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.1387 - val_loss: 0.1613\n",
      "Epoch 30/40\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.1359 - val_loss: 0.1597\n",
      "Epoch 31/40\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.1374 - val_loss: 0.1592\n",
      "Epoch 32/40\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.1374 - val_loss: 0.1600\n",
      "Epoch 33/40\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.1354 - val_loss: 0.1592\n",
      "Epoch 34/40\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.1364 - val_loss: 0.1590\n",
      "Epoch 35/40\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.1371 - val_loss: 0.1601\n",
      "Epoch 36/40\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.1357 - val_loss: 0.1590\n",
      "Epoch 37/40\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.1365 - val_loss: 0.1587\n",
      "Epoch 38/40\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.1359 - val_loss: 0.1597\n",
      "Epoch 39/40\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.1355 - val_loss: 0.1586\n",
      "Epoch 40/40\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.1359 - val_loss: 0.1584\n"
     ]
    }
   ],
   "source": [
    "# Stacked GRU Model - for deeper architecture\n",
    "# 2 layer 64/64 units\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.GRU(64,\n",
    "                     return_sequences=True,\n",
    "                     input_shape=(None, spd_mat.shape[-1])))\n",
    "model.add(layers.GRU(64, activation='relu'))\n",
    "model.add(layers.Dense(n_out))\n",
    "\n",
    "model.compile(optimizer=RMSprop(decay=0.001), loss=root_mean_squared_error)\n",
    "model.summary()\n",
    "history = model.fit_generator(gen_tr,\n",
    "                              steps_per_epoch=500,\n",
    "                              epochs=40,\n",
    "                              validation_data=gen_val,\n",
    "                              validation_steps=val_steps)\n",
    "model.save('/content/gdrive/My Drive/NNDL-project/gru_2_layer_64_64_3step_rmse.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 338147,
     "status": "ok",
     "timestamp": 1576344378589,
     "user": {
      "displayName": "Sandeep Mudigonda",
      "photoUrl": "",
      "userId": "02295463398705014066"
     },
     "user_tz": 300
    },
    "id": "wvFHdyZ6_sh-",
    "outputId": "2ba00d7f-fc54-4651-d44a-386ca8aa687d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_13\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_19 (GRU)                 (None, None, 128)         129024    \n",
      "_________________________________________________________________\n",
      "gru_20 (GRU)                 (None, 64)                37056     \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 207)               13455     \n",
      "=================================================================\n",
      "Total params: 179,535\n",
      "Trainable params: 179,535\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/40\n",
      "500/500 [==============================] - 13s 26ms/step - loss: 0.1959 - val_loss: 0.2082\n",
      "Epoch 2/40\n",
      "500/500 [==============================] - 8s 17ms/step - loss: 0.1641 - val_loss: 0.1764\n",
      "Epoch 3/40\n",
      "500/500 [==============================] - 8s 17ms/step - loss: 0.1597 - val_loss: 0.1735\n",
      "Epoch 4/40\n",
      "500/500 [==============================] - 8s 17ms/step - loss: 0.1544 - val_loss: 0.1804\n",
      "Epoch 5/40\n",
      "500/500 [==============================] - 8s 17ms/step - loss: 0.1486 - val_loss: 0.1752\n",
      "Epoch 6/40\n",
      "500/500 [==============================] - 8s 17ms/step - loss: 0.1480 - val_loss: 0.1675\n",
      "Epoch 7/40\n",
      "500/500 [==============================] - 8s 17ms/step - loss: 0.1465 - val_loss: 0.1691\n",
      "Epoch 8/40\n",
      "500/500 [==============================] - 8s 17ms/step - loss: 0.1431 - val_loss: 0.1682\n",
      "Epoch 9/40\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.1429 - val_loss: 0.1652\n",
      "Epoch 10/40\n",
      "500/500 [==============================] - 8s 17ms/step - loss: 0.1411 - val_loss: 0.1632\n",
      "Epoch 11/40\n",
      "500/500 [==============================] - 8s 17ms/step - loss: 0.1398 - val_loss: 0.1656\n",
      "Epoch 12/40\n",
      "500/500 [==============================] - 8s 17ms/step - loss: 0.1395 - val_loss: 0.1636\n",
      "Epoch 13/40\n",
      "500/500 [==============================] - 8s 17ms/step - loss: 0.1378 - val_loss: 0.1619\n",
      "Epoch 14/40\n",
      "500/500 [==============================] - 8s 17ms/step - loss: 0.1381 - val_loss: 0.1635\n",
      "Epoch 15/40\n",
      "500/500 [==============================] - 8s 17ms/step - loss: 0.1369 - val_loss: 0.1606\n",
      "Epoch 16/40\n",
      "500/500 [==============================] - 8s 17ms/step - loss: 0.1355 - val_loss: 0.1604\n",
      "Epoch 17/40\n",
      "500/500 [==============================] - 8s 17ms/step - loss: 0.1369 - val_loss: 0.1652\n",
      "Epoch 18/40\n",
      "500/500 [==============================] - 8s 17ms/step - loss: 0.1344 - val_loss: 0.1592\n",
      "Epoch 19/40\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.1348 - val_loss: 0.1593\n",
      "Epoch 20/40\n",
      "500/500 [==============================] - 8s 17ms/step - loss: 0.1347 - val_loss: 0.1602\n",
      "Epoch 21/40\n",
      "500/500 [==============================] - 8s 17ms/step - loss: 0.1332 - val_loss: 0.1602\n",
      "Epoch 22/40\n",
      "500/500 [==============================] - 8s 17ms/step - loss: 0.1340 - val_loss: 0.1598\n",
      "Epoch 23/40\n",
      "500/500 [==============================] - 8s 17ms/step - loss: 0.1337 - val_loss: 0.1572\n",
      "Epoch 24/40\n",
      "500/500 [==============================] - 8s 17ms/step - loss: 0.1322 - val_loss: 0.1582\n",
      "Epoch 25/40\n",
      "500/500 [==============================] - 8s 17ms/step - loss: 0.1330 - val_loss: 0.1573\n",
      "Epoch 26/40\n",
      "500/500 [==============================] - 8s 17ms/step - loss: 0.1319 - val_loss: 0.1570\n",
      "Epoch 27/40\n",
      "500/500 [==============================] - 8s 17ms/step - loss: 0.1316 - val_loss: 0.1582\n",
      "Epoch 28/40\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.1320 - val_loss: 0.1563\n",
      "Epoch 29/40\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.1309 - val_loss: 0.1575\n",
      "Epoch 30/40\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.1319 - val_loss: 0.1578\n",
      "Epoch 31/40\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.1309 - val_loss: 0.1565\n",
      "Epoch 32/40\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.1302 - val_loss: 0.1564\n",
      "Epoch 33/40\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.1313 - val_loss: 0.1573\n",
      "Epoch 34/40\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.1296 - val_loss: 0.1558\n",
      "Epoch 35/40\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.1303 - val_loss: 0.1565\n",
      "Epoch 36/40\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.1300 - val_loss: 0.1561\n",
      "Epoch 37/40\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.1294 - val_loss: 0.1565\n",
      "Epoch 38/40\n",
      "500/500 [==============================] - 8s 17ms/step - loss: 0.1299 - val_loss: 0.1565\n",
      "Epoch 39/40\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.1299 - val_loss: 0.1549\n",
      "Epoch 40/40\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.1286 - val_loss: 0.1559\n"
     ]
    }
   ],
   "source": [
    "# Stacked GRU Model - for deeper architecture\n",
    "# 2 layer 128/64 units\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.GRU(128,\n",
    "                     return_sequences=True,\n",
    "                     input_shape=(None, spd_mat.shape[-1])))\n",
    "model.add(layers.GRU(64, activation='relu'))\n",
    "model.add(layers.Dense(n_out))\n",
    "\n",
    "model.compile(optimizer=RMSprop(decay=0.001), loss=root_mean_squared_error)\n",
    "model.summary()\n",
    "m4 = model\n",
    "history = m4.fit_generator(gen_tr,\n",
    "                              steps_per_epoch=500,\n",
    "                              epochs=40,\n",
    "                              validation_data=gen_val,\n",
    "                              validation_steps=val_steps)\n",
    "#model.save('/content/gdrive/My Drive/NNDL-project/gru_2_layer_128_64_Std_3step_rmse.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 335743,
     "status": "ok",
     "timestamp": 1576341097496,
     "user": {
      "displayName": "Sandeep Mudigonda",
      "photoUrl": "",
      "userId": "02295463398705014066"
     },
     "user_tz": 300
    },
    "id": "0b_FFA7DBwa7",
    "outputId": "d046af9d-00b3-4828-8904-5f65b7515f8b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_13 (GRU)                 (None, None, 128)         129024    \n",
      "_________________________________________________________________\n",
      "gru_14 (GRU)                 (None, 64)                37056     \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 207)               13455     \n",
      "=================================================================\n",
      "Total params: 179,535\n",
      "Trainable params: 179,535\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/40\n",
      "500/500 [==============================] - 11s 22ms/step - loss: 0.2093 - val_loss: 0.1887\n",
      "Epoch 2/40\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.1745 - val_loss: 0.1822\n",
      "Epoch 3/40\n",
      "500/500 [==============================] - 8s 17ms/step - loss: 0.1681 - val_loss: 0.1822\n",
      "Epoch 4/40\n",
      "500/500 [==============================] - 8s 17ms/step - loss: 0.1619 - val_loss: 0.1749\n",
      "Epoch 5/40\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.1634 - val_loss: 0.1739\n",
      "Epoch 6/40\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.1610 - val_loss: 0.1738\n",
      "Epoch 7/40\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.1564 - val_loss: 0.1731\n",
      "Epoch 8/40\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.1570 - val_loss: 0.1710\n",
      "Epoch 9/40\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.1547 - val_loss: 0.1709\n",
      "Epoch 10/40\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.1516 - val_loss: 0.1715\n",
      "Epoch 11/40\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.1519 - val_loss: 0.1683\n",
      "Epoch 12/40\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.1501 - val_loss: 0.1690\n",
      "Epoch 13/40\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.1485 - val_loss: 0.1688\n",
      "Epoch 14/40\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.1486 - val_loss: 0.1669\n",
      "Epoch 15/40\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.1467 - val_loss: 0.1675\n",
      "Epoch 16/40\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.1473 - val_loss: 0.1688\n",
      "Epoch 17/40\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.1448 - val_loss: 0.1658\n",
      "Epoch 18/40\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.1455 - val_loss: 0.1673\n",
      "Epoch 19/40\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.1448 - val_loss: 0.1691\n",
      "Epoch 20/40\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.1424 - val_loss: 0.1651\n",
      "Epoch 21/40\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.1432 - val_loss: 0.1652\n",
      "Epoch 22/40\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.1431 - val_loss: 0.1665\n",
      "Epoch 23/40\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.1418 - val_loss: 0.1655\n",
      "Epoch 24/40\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.1421 - val_loss: 0.1651\n",
      "Epoch 25/40\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.1410 - val_loss: 0.1650\n",
      "Epoch 26/40\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.1401 - val_loss: 0.1649\n",
      "Epoch 27/40\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.1410 - val_loss: 0.1639\n",
      "Epoch 28/40\n",
      "500/500 [==============================] - 8s 17ms/step - loss: 0.1396 - val_loss: 0.1644\n",
      "Epoch 29/40\n",
      "500/500 [==============================] - 8s 17ms/step - loss: 0.1394 - val_loss: 0.1638\n",
      "Epoch 30/40\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.1398 - val_loss: 0.1634\n",
      "Epoch 31/40\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.1382 - val_loss: 0.1639\n",
      "Epoch 32/40\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.1400 - val_loss: 0.1643\n",
      "Epoch 33/40\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.1376 - val_loss: 0.1627\n",
      "Epoch 34/40\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.1384 - val_loss: 0.1632\n",
      "Epoch 35/40\n",
      "500/500 [==============================] - 8s 17ms/step - loss: 0.1383 - val_loss: 0.1644\n",
      "Epoch 36/40\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.1369 - val_loss: 0.1623\n",
      "Epoch 37/40\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.1372 - val_loss: 0.1623\n",
      "Epoch 38/40\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.1377 - val_loss: 0.1632\n",
      "Epoch 39/40\n",
      "500/500 [==============================] - 8s 17ms/step - loss: 0.1371 - val_loss: 0.1625\n",
      "Epoch 40/40\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.1371 - val_loss: 0.1622\n"
     ]
    }
   ],
   "source": [
    "# Stacked GRU Model - for deeper architecture\n",
    "# 2 layer 128/64 units\n",
    "# Adamax\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.optimizers import Adam, Adamax\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.GRU(128,\n",
    "                     return_sequences=True,\n",
    "                     input_shape=(None, spd_mat.shape[-1])))\n",
    "model.add(layers.GRU(64, activation='relu'))\n",
    "model.add(layers.Dense(n_out))\n",
    "\n",
    "model.compile(optimizer=Adamax(decay=0.001), loss=root_mean_squared_error)\n",
    "model.summary()\n",
    "history = model.fit_generator(gen_tr,\n",
    "                              steps_per_epoch=500,\n",
    "                              epochs=40,\n",
    "                              validation_data=gen_val,\n",
    "                              validation_steps=val_steps)\n",
    "model.save('/content/gdrive/My Drive/NNDL-project/gru_2_layer_128_64_adamax_3step_rmse.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 400543,
     "status": "ok",
     "timestamp": 1576338179063,
     "user": {
      "displayName": "Sandeep Mudigonda",
      "photoUrl": "",
      "userId": "02295463398705014066"
     },
     "user_tz": 300
    },
    "id": "3PPNmBhTB1bu",
    "outputId": "db5a9548-991c-4a51-8b71-b1bf75ca65ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_2 (LSTM)                (None, None, 128)         172032    \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 64)                49408     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 207)               13455     \n",
      "=================================================================\n",
      "Total params: 234,895\n",
      "Trainable params: 234,895\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/40\n",
      "500/500 [==============================] - 12s 24ms/step - loss: 0.2066 - val_loss: 0.2055\n",
      "Epoch 2/40\n",
      "500/500 [==============================] - 10s 20ms/step - loss: 0.1703 - val_loss: 0.1799\n",
      "Epoch 3/40\n",
      "500/500 [==============================] - 10s 21ms/step - loss: 0.1645 - val_loss: 0.1783\n",
      "Epoch 4/40\n",
      "500/500 [==============================] - 12s 23ms/step - loss: 0.1612 - val_loss: 0.1807\n",
      "Epoch 5/40\n",
      "500/500 [==============================] - 11s 22ms/step - loss: 0.1560 - val_loss: 0.1709\n",
      "Epoch 6/40\n",
      "500/500 [==============================] - 10s 21ms/step - loss: 0.1542 - val_loss: 0.1839\n",
      "Epoch 7/40\n",
      "500/500 [==============================] - 10s 21ms/step - loss: 0.1524 - val_loss: 0.1751\n",
      "Epoch 8/40\n",
      "500/500 [==============================] - 10s 20ms/step - loss: 0.1485 - val_loss: 0.1682\n",
      "Epoch 9/40\n",
      "500/500 [==============================] - 10s 21ms/step - loss: 0.1489 - val_loss: 0.1708\n",
      "Epoch 10/40\n",
      "500/500 [==============================] - 10s 20ms/step - loss: 0.1473 - val_loss: 0.1682\n",
      "Epoch 11/40\n",
      "500/500 [==============================] - 10s 20ms/step - loss: 0.1451 - val_loss: 0.1679\n",
      "Epoch 12/40\n",
      "500/500 [==============================] - 10s 19ms/step - loss: 0.1456 - val_loss: 0.1688\n",
      "Epoch 13/40\n",
      "500/500 [==============================] - 10s 19ms/step - loss: 0.1438 - val_loss: 0.1658\n",
      "Epoch 14/40\n",
      "500/500 [==============================] - 10s 19ms/step - loss: 0.1424 - val_loss: 0.1696\n",
      "Epoch 15/40\n",
      "500/500 [==============================] - 10s 19ms/step - loss: 0.1425 - val_loss: 0.1649\n",
      "Epoch 16/40\n",
      "500/500 [==============================] - 10s 20ms/step - loss: 0.1410 - val_loss: 0.1634\n",
      "Epoch 17/40\n",
      "500/500 [==============================] - 10s 20ms/step - loss: 0.1405 - val_loss: 0.1638\n",
      "Epoch 18/40\n",
      "500/500 [==============================] - 10s 20ms/step - loss: 0.1406 - val_loss: 0.1626\n",
      "Epoch 19/40\n",
      "500/500 [==============================] - 10s 19ms/step - loss: 0.1389 - val_loss: 0.1634\n",
      "Epoch 20/40\n",
      "500/500 [==============================] - 10s 19ms/step - loss: 0.1404 - val_loss: 0.1648\n",
      "Epoch 21/40\n",
      "500/500 [==============================] - 10s 19ms/step - loss: 0.1380 - val_loss: 0.1637\n",
      "Epoch 22/40\n",
      "500/500 [==============================] - 10s 20ms/step - loss: 0.1388 - val_loss: 0.1626\n",
      "Epoch 23/40\n",
      "500/500 [==============================] - 10s 19ms/step - loss: 0.1385 - val_loss: 0.1627\n",
      "Epoch 24/40\n",
      "500/500 [==============================] - 10s 19ms/step - loss: 0.1368 - val_loss: 0.1619\n",
      "Epoch 25/40\n",
      "500/500 [==============================] - 10s 19ms/step - loss: 0.1376 - val_loss: 0.1612\n",
      "Epoch 26/40\n",
      "500/500 [==============================] - 10s 19ms/step - loss: 0.1376 - val_loss: 0.1619\n",
      "Epoch 27/40\n",
      "500/500 [==============================] - 10s 19ms/step - loss: 0.1367 - val_loss: 0.1621\n",
      "Epoch 28/40\n",
      "500/500 [==============================] - 10s 19ms/step - loss: 0.1374 - val_loss: 0.1611\n",
      "Epoch 29/40\n",
      "500/500 [==============================] - 10s 19ms/step - loss: 0.1363 - val_loss: 0.1608\n",
      "Epoch 30/40\n",
      "500/500 [==============================] - 10s 19ms/step - loss: 0.1360 - val_loss: 0.1620\n",
      "Epoch 31/40\n",
      "500/500 [==============================] - 10s 19ms/step - loss: 0.1365 - val_loss: 0.1608\n",
      "Epoch 32/40\n",
      "500/500 [==============================] - 10s 19ms/step - loss: 0.1357 - val_loss: 0.1611\n",
      "Epoch 33/40\n",
      "500/500 [==============================] - 10s 19ms/step - loss: 0.1352 - val_loss: 0.1606\n",
      "Epoch 34/40\n",
      "500/500 [==============================] - 10s 19ms/step - loss: 0.1361 - val_loss: 0.1601\n",
      "Epoch 35/40\n",
      "500/500 [==============================] - 10s 19ms/step - loss: 0.1347 - val_loss: 0.1605\n",
      "Epoch 36/40\n",
      "500/500 [==============================] - 10s 19ms/step - loss: 0.1363 - val_loss: 0.1621\n",
      "Epoch 37/40\n",
      "500/500 [==============================] - 10s 19ms/step - loss: 0.1342 - val_loss: 0.1607\n",
      "Epoch 38/40\n",
      "500/500 [==============================] - 10s 19ms/step - loss: 0.1353 - val_loss: 0.1602\n",
      "Epoch 39/40\n",
      "500/500 [==============================] - 10s 19ms/step - loss: 0.1352 - val_loss: 0.1605\n",
      "Epoch 40/40\n",
      "500/500 [==============================] - 10s 19ms/step - loss: 0.1335 - val_loss: 0.1606\n"
     ]
    }
   ],
   "source": [
    "# Stacked LSTM Model - for deeper architecture\n",
    "# 2 layer 128/64 units\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.LSTM(128,\n",
    "                     return_sequences=True,\n",
    "                     input_shape=(None, spd_mat.shape[-1])))\n",
    "model.add(layers.LSTM(64, activation='relu'))\n",
    "model.add(layers.Dense(n_out))\n",
    "\n",
    "model.compile(optimizer=RMSprop(decay=0.001), loss=root_mean_squared_error)\n",
    "model.summary()\n",
    "history = model.fit_generator(gen_tr,\n",
    "                              steps_per_epoch=500,\n",
    "                              epochs=40,\n",
    "                              validation_data=gen_val,\n",
    "                              validation_steps=val_steps)\n",
    "model.save('/content/gdrive/My Drive/NNDL-project/lstm_2_layer_128_64_Std_3step_rmse.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 360350,
     "status": "ok",
     "timestamp": 1576338765740,
     "user": {
      "displayName": "Sandeep Mudigonda",
      "photoUrl": "",
      "userId": "02295463398705014066"
     },
     "user_tz": 300
    },
    "id": "6Fll1TTR_tHP",
    "outputId": "34fcedd0-b373-4b7d-f964-c2a3020b2b40"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_8 (GRU)                  (None, None, 256)         356352    \n",
      "_________________________________________________________________\n",
      "gru_9 (GRU)                  (None, 128)               147840    \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 207)               26703     \n",
      "=================================================================\n",
      "Total params: 530,895\n",
      "Trainable params: 530,895\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/40\n",
      "500/500 [==============================] - 11s 21ms/step - loss: 0.1931 - val_loss: 0.1892\n",
      "Epoch 2/40\n",
      "500/500 [==============================] - 9s 17ms/step - loss: 0.1623 - val_loss: 0.1746\n",
      "Epoch 3/40\n",
      "500/500 [==============================] - 9s 17ms/step - loss: 0.1530 - val_loss: 0.2066\n",
      "Epoch 4/40\n",
      "500/500 [==============================] - 9s 17ms/step - loss: 0.1484 - val_loss: 0.1706\n",
      "Epoch 5/40\n",
      "500/500 [==============================] - 9s 17ms/step - loss: 0.1430 - val_loss: 0.1659\n",
      "Epoch 6/40\n",
      "500/500 [==============================] - 9s 17ms/step - loss: 0.1395 - val_loss: 0.1642\n",
      "Epoch 7/40\n",
      "500/500 [==============================] - 9s 17ms/step - loss: 0.1380 - val_loss: 0.1603\n",
      "Epoch 8/40\n",
      "500/500 [==============================] - 9s 17ms/step - loss: 0.1344 - val_loss: 0.1594\n",
      "Epoch 9/40\n",
      "500/500 [==============================] - 9s 17ms/step - loss: 0.1346 - val_loss: 0.1683\n",
      "Epoch 10/40\n",
      "500/500 [==============================] - 9s 18ms/step - loss: 0.1313 - val_loss: 0.1592\n",
      "Epoch 11/40\n",
      "500/500 [==============================] - 9s 18ms/step - loss: 0.1309 - val_loss: 0.1599\n",
      "Epoch 12/40\n",
      "500/500 [==============================] - 9s 18ms/step - loss: 0.1298 - val_loss: 0.1565\n",
      "Epoch 13/40\n",
      "500/500 [==============================] - 9s 18ms/step - loss: 0.1280 - val_loss: 0.1579\n",
      "Epoch 14/40\n",
      "500/500 [==============================] - 9s 18ms/step - loss: 0.1277 - val_loss: 0.1538\n",
      "Epoch 15/40\n",
      "500/500 [==============================] - 9s 18ms/step - loss: 0.1272 - val_loss: 0.1530\n",
      "Epoch 16/40\n",
      "500/500 [==============================] - 9s 18ms/step - loss: 0.1263 - val_loss: 0.1568\n",
      "Epoch 17/40\n",
      "500/500 [==============================] - 9s 18ms/step - loss: 0.1261 - val_loss: 0.1524\n",
      "Epoch 18/40\n",
      "500/500 [==============================] - 9s 18ms/step - loss: 0.1249 - val_loss: 0.1547\n",
      "Epoch 19/40\n",
      "500/500 [==============================] - 9s 18ms/step - loss: 0.1240 - val_loss: 0.1514\n",
      "Epoch 20/40\n",
      "500/500 [==============================] - 9s 18ms/step - loss: 0.1250 - val_loss: 0.1508\n",
      "Epoch 21/40\n",
      "500/500 [==============================] - 9s 18ms/step - loss: 0.1234 - val_loss: 0.1517\n",
      "Epoch 22/40\n",
      "500/500 [==============================] - 9s 18ms/step - loss: 0.1232 - val_loss: 0.1501\n",
      "Epoch 23/40\n",
      "500/500 [==============================] - 9s 18ms/step - loss: 0.1238 - val_loss: 0.1519\n",
      "Epoch 24/40\n",
      "500/500 [==============================] - 9s 18ms/step - loss: 0.1219 - val_loss: 0.1510\n",
      "Epoch 25/40\n",
      "500/500 [==============================] - 9s 18ms/step - loss: 0.1233 - val_loss: 0.1523\n",
      "Epoch 26/40\n",
      "500/500 [==============================] - 9s 18ms/step - loss: 0.1219 - val_loss: 0.1491\n",
      "Epoch 27/40\n",
      "500/500 [==============================] - 9s 18ms/step - loss: 0.1221 - val_loss: 0.1490\n",
      "Epoch 28/40\n",
      "500/500 [==============================] - 9s 18ms/step - loss: 0.1219 - val_loss: 0.1502\n",
      "Epoch 29/40\n",
      "500/500 [==============================] - 9s 18ms/step - loss: 0.1209 - val_loss: 0.1524\n",
      "Epoch 30/40\n",
      "500/500 [==============================] - 9s 18ms/step - loss: 0.1214 - val_loss: 0.1489\n",
      "Epoch 31/40\n",
      "500/500 [==============================] - 9s 18ms/step - loss: 0.1216 - val_loss: 0.1510\n",
      "Epoch 32/40\n",
      "500/500 [==============================] - 9s 18ms/step - loss: 0.1205 - val_loss: 0.1498\n",
      "Epoch 33/40\n",
      "500/500 [==============================] - 9s 18ms/step - loss: 0.1211 - val_loss: 0.1480\n",
      "Epoch 34/40\n",
      "500/500 [==============================] - 9s 18ms/step - loss: 0.1206 - val_loss: 0.1489\n",
      "Epoch 35/40\n",
      "500/500 [==============================] - 9s 18ms/step - loss: 0.1196 - val_loss: 0.1493\n",
      "Epoch 36/40\n",
      "500/500 [==============================] - 9s 18ms/step - loss: 0.1211 - val_loss: 0.1479\n",
      "Epoch 37/40\n",
      "500/500 [==============================] - 9s 18ms/step - loss: 0.1196 - val_loss: 0.1478\n",
      "Epoch 38/40\n",
      "500/500 [==============================] - 9s 18ms/step - loss: 0.1200 - val_loss: 0.1479\n",
      "Epoch 39/40\n",
      "500/500 [==============================] - 9s 18ms/step - loss: 0.1203 - val_loss: 0.1483\n",
      "Epoch 40/40\n",
      "500/500 [==============================] - 9s 18ms/step - loss: 0.1187 - val_loss: 0.1487\n"
     ]
    }
   ],
   "source": [
    "# Stacked GRU Model - for deeper architecture\n",
    "# 2 layer 256/128 units\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.GRU(256,\n",
    "                     return_sequences=True,\n",
    "                     input_shape=(None, spd_mat.shape[-1])))\n",
    "model.add(layers.GRU(128, activation='relu'))\n",
    "model.add(layers.Dense(n_out))\n",
    "\n",
    "model.compile(optimizer=RMSprop(decay=0.001), loss=root_mean_squared_error)\n",
    "model.summary()\n",
    "history = model.fit_generator(gen_tr,\n",
    "                              steps_per_epoch=500,\n",
    "                              epochs=40,\n",
    "                              validation_data=gen_val,\n",
    "                              validation_steps=val_steps)\n",
    "model.save('/content/gdrive/My Drive/NNDL-project/gru_2_layer_256_128_Std_3step_rmse.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 504974,
     "status": "ok",
     "timestamp": 1576344002837,
     "user": {
      "displayName": "Sandeep Mudigonda",
      "photoUrl": "",
      "userId": "02295463398705014066"
     },
     "user_tz": 300
    },
    "id": "_IH9jKrqRPoh",
    "outputId": "15947aa6-4942-40ae-bd9a-2405488957d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_16 (GRU)                 (None, None, 256)         356352    \n",
      "_________________________________________________________________\n",
      "gru_17 (GRU)                 (None, None, 256)         393984    \n",
      "_________________________________________________________________\n",
      "gru_18 (GRU)                 (None, 128)               147840    \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 207)               26703     \n",
      "=================================================================\n",
      "Total params: 924,879\n",
      "Trainable params: 924,879\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/40\n",
      "500/500 [==============================] - 17s 34ms/step - loss: 0.1964 - val_loss: 0.2094\n",
      "Epoch 2/40\n",
      "500/500 [==============================] - 12s 25ms/step - loss: 0.1670 - val_loss: 0.1816\n",
      "Epoch 3/40\n",
      "500/500 [==============================] - 12s 25ms/step - loss: 0.1571 - val_loss: 0.1870\n",
      "Epoch 4/40\n",
      "500/500 [==============================] - 12s 25ms/step - loss: 0.1532 - val_loss: 0.1729\n",
      "Epoch 5/40\n",
      "500/500 [==============================] - 12s 25ms/step - loss: 0.1474 - val_loss: 0.1704\n",
      "Epoch 6/40\n",
      "500/500 [==============================] - 12s 25ms/step - loss: 0.1431 - val_loss: 0.1848\n",
      "Epoch 7/40\n",
      "500/500 [==============================] - 13s 25ms/step - loss: 0.1413 - val_loss: 0.1651\n",
      "Epoch 8/40\n",
      "500/500 [==============================] - 13s 25ms/step - loss: 0.1378 - val_loss: 0.1633\n",
      "Epoch 9/40\n",
      "500/500 [==============================] - 12s 25ms/step - loss: 0.1353 - val_loss: 0.1626\n",
      "Epoch 10/40\n",
      "500/500 [==============================] - 12s 25ms/step - loss: 0.1352 - val_loss: 0.1610\n",
      "Epoch 11/40\n",
      "500/500 [==============================] - 12s 25ms/step - loss: 0.1321 - val_loss: 0.1598\n",
      "Epoch 12/40\n",
      "500/500 [==============================] - 12s 25ms/step - loss: 0.1326 - val_loss: 0.1674\n",
      "Epoch 13/40\n",
      "500/500 [==============================] - 12s 25ms/step - loss: 0.1303 - val_loss: 0.1607\n",
      "Epoch 14/40\n",
      "500/500 [==============================] - 12s 25ms/step - loss: 0.1298 - val_loss: 0.1576\n",
      "Epoch 15/40\n",
      "500/500 [==============================] - 12s 25ms/step - loss: 0.1291 - val_loss: 0.1573\n",
      "Epoch 16/40\n",
      "500/500 [==============================] - 12s 25ms/step - loss: 0.1273 - val_loss: 0.1565\n",
      "Epoch 17/40\n",
      "500/500 [==============================] - 12s 25ms/step - loss: 0.1271 - val_loss: 0.1575\n",
      "Epoch 18/40\n",
      "500/500 [==============================] - 12s 25ms/step - loss: 0.1269 - val_loss: 0.1552\n",
      "Epoch 19/40\n",
      "500/500 [==============================] - 12s 25ms/step - loss: 0.1260 - val_loss: 0.1559\n",
      "Epoch 20/40\n",
      "500/500 [==============================] - 12s 25ms/step - loss: 0.1262 - val_loss: 0.1551\n",
      "Epoch 21/40\n",
      "500/500 [==============================] - 12s 25ms/step - loss: 0.1249 - val_loss: 0.1575\n",
      "Epoch 22/40\n",
      "500/500 [==============================] - 12s 25ms/step - loss: 0.1240 - val_loss: 0.1538\n",
      "Epoch 23/40\n",
      "500/500 [==============================] - 12s 25ms/step - loss: 0.1250 - val_loss: 0.1539\n",
      "Epoch 24/40\n",
      "500/500 [==============================] - 12s 25ms/step - loss: 0.1235 - val_loss: 0.1529\n",
      "Epoch 25/40\n",
      "500/500 [==============================] - 12s 25ms/step - loss: 0.1229 - val_loss: 0.1526\n",
      "Epoch 26/40\n",
      "500/500 [==============================] - 12s 25ms/step - loss: 0.1240 - val_loss: 0.1526\n",
      "Epoch 27/40\n",
      "500/500 [==============================] - 12s 25ms/step - loss: 0.1220 - val_loss: 0.1525\n",
      "Epoch 28/40\n",
      "500/500 [==============================] - 12s 25ms/step - loss: 0.1236 - val_loss: 0.1544\n",
      "Epoch 29/40\n",
      "500/500 [==============================] - 12s 25ms/step - loss: 0.1219 - val_loss: 0.1528\n",
      "Epoch 30/40\n",
      "500/500 [==============================] - 12s 25ms/step - loss: 0.1222 - val_loss: 0.1519\n",
      "Epoch 31/40\n",
      "500/500 [==============================] - 13s 25ms/step - loss: 0.1220 - val_loss: 0.1530\n",
      "Epoch 32/40\n",
      "500/500 [==============================] - 13s 26ms/step - loss: 0.1210 - val_loss: 0.1518\n",
      "Epoch 33/40\n",
      "500/500 [==============================] - 13s 25ms/step - loss: 0.1212 - val_loss: 0.1516\n",
      "Epoch 34/40\n",
      "500/500 [==============================] - 12s 25ms/step - loss: 0.1214 - val_loss: 0.1523\n",
      "Epoch 35/40\n",
      "500/500 [==============================] - 12s 25ms/step - loss: 0.1207 - val_loss: 0.1536\n",
      "Epoch 36/40\n",
      "500/500 [==============================] - 12s 25ms/step - loss: 0.1211 - val_loss: 0.1512\n",
      "Epoch 37/40\n",
      "500/500 [==============================] - 12s 25ms/step - loss: 0.1205 - val_loss: 0.1530\n",
      "Epoch 38/40\n",
      "500/500 [==============================] - 12s 25ms/step - loss: 0.1196 - val_loss: 0.1507\n",
      "Epoch 39/40\n",
      "500/500 [==============================] - 13s 25ms/step - loss: 0.1210 - val_loss: 0.1516\n",
      "Epoch 40/40\n",
      "500/500 [==============================] - 12s 25ms/step - loss: 0.1195 - val_loss: 0.1502\n"
     ]
    }
   ],
   "source": [
    "# Stacked GRU 3 layer 256/256/128 units\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.GRU(256,\n",
    "                     return_sequences=True,\n",
    "                     input_shape=(None, spd_mat.shape[-1])))\n",
    "model.add(layers.GRU(256,\n",
    "                     return_sequences=True,\n",
    "                     input_shape=(None, spd_mat.shape[-1])))\n",
    "model.add(layers.GRU(128, activation='relu'))\n",
    "model.add(layers.Dense(n_out))\n",
    "\n",
    "model.compile(optimizer=RMSprop(decay=0.001), loss=root_mean_squared_error)\n",
    "model.summary()\n",
    "m3 = model\n",
    "history = m3.fit_generator(gen_tr,\n",
    "                              steps_per_epoch=500,\n",
    "                              epochs=40,\n",
    "                              validation_data=gen_val,\n",
    "                              validation_steps=val_steps)\n",
    "#model.save('/content/gdrive/My Drive/NNDL-project/gru_3_layer_256_256_128_Std_3step_rmse.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F7d6pHF0NzBe"
   },
   "source": [
    "## Comparing Prediciton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rDaPY84V9WO4"
   },
   "outputs": [],
   "source": [
    "# Prediction on test set\n",
    "\n",
    "from keras.models import load_model \n",
    "\n",
    "m0 = load_model('/content/gdrive/My Drive/NNDL-project/fnn_2_layer_64_Std_3step_full.h5')\n",
    "m1 = load_model('/content/gdrive/My Drive/NNDL-project/gru_1_layer_simple_64_3step_full.h5')\n",
    "m2 = load_model('/content/gdrive/My Drive/NNDL-project/gru_2_layer_128_64_Std_3step_full.h5')\n",
    "m3 = load_model('/content/gdrive/My Drive/NNDL-project/gru_2_layer_128_64_12Lag.h5')\n",
    "m4 = load_model('/content/gdrive/My Drive/NNDL-project/gru_2_layer_128_64_64Lag.h5')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2EYA--ikNB5T"
   },
   "outputs": [],
   "source": [
    "\n",
    "test_pred0 = m0.predict_generator(gen_tes, steps=test_steps)\n",
    "test_pred1 = m1.predict_generator(gen_tes, steps=test_steps)\n",
    "test_pred2 = m2.predict_generator(gen_tes, steps=test_steps)\n",
    "test_pred3 = m3.predict_generator(gen_tes_12, steps=test_steps_12)\n",
    "test_pred4 = m4.predict_generator(gen_tes_64, steps=test_steps_64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "abPtexe5Tjlt"
   },
   "outputs": [],
   "source": [
    "# Prediction on test set using different MAE & RMSE models\n",
    "\n",
    "from keras.models import load_model \n",
    "\n",
    "#m0 = load_model('/content/gdrive/My Drive/NNDL-project/gru_2_layer_128_64_Std_3step_full.h5')\n",
    "m1 = model #load_model('/content/gdrive/My Drive/NNDL-project/lstm_1_layer_128_Std_3step_rmse.h5')\n",
    "m2 = load_model('/content/gdrive/My Drive/NNDL-project/gru_1_layer_128_3step_rmse.h5')\n",
    "m3 = load_model('/content/gdrive/My Drive/NNDL-project/gru_3_layer_256_256_128_Std_3step_rmse.h5')\n",
    "m4 = load_model('/content/gdrive/My Drive/NNDL-project/gru_2_layer_128_64_Std_3step_rmse.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gWtcF9L9UK93"
   },
   "outputs": [],
   "source": [
    "\n",
    "test_pred0_mvr = m0.predict_generator(gen_tes, steps=test_steps)\n",
    "test_pred1_mvr = m1.predict_generator(gen_tes, steps=test_steps)\n",
    "test_pred2_mvr = m2.predict_generator(gen_tes, steps=test_steps)\n",
    "test_pred3_mvr = m3.predict_generator(gen_tes, steps=test_steps)\n",
    "test_pred4_mvr = m4.predict_generator(gen_tes, steps=test_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 396,
     "status": "ok",
     "timestamp": 1575840922271,
     "user": {
      "displayName": "Sandeep Mudigonda",
      "photoUrl": "",
      "userId": "02295463398705014066"
     },
     "user_tz": 300
    },
    "id": "xLTo2YeI56ih",
    "outputId": "d3106763-55ea-42b7-a186-fba4749f5d67"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3712, 207)\n",
      "0.24681336269955412\n",
      "0.2765968770823736\n",
      "0.2598719837401443\n",
      "0.26820819427270565\n",
      "0.2663786771825922\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "print(test_pred0.shape)\n",
    "print(mean_absolute_error(test_pred0, spd_mat[30529:30529+3712,]))\n",
    "print(mean_absolute_error(test_pred1, spd_mat[30529:30529+3712,]))\n",
    "print(mean_absolute_error(test_pred2, spd_mat[30529:30529+3712,]))\n",
    "print(mean_absolute_error(test_pred3, spd_mat[30529:30529+3712,]))\n",
    "print(mean_absolute_error(test_pred4, spd_mat[30529:30529+3584,]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EfDZASdl_FSq"
   },
   "outputs": [],
   "source": [
    "\n",
    "abs_err0 = np.absolute(test_pred0_mvr, spd_mat[30529:30529+3712,])\n",
    "mean_abs_err0 = np.mean(abs_err0,axis=0)\n",
    "#print(mean_abs_err1.shape)\n",
    "#print(np.var(abs_err1,axis=1))\n",
    "\n",
    "abs_err1 = np.absolute(test_pred1_mvr, spd_mat[30529:30529+3712,])\n",
    "mean_abs_err1 = np.mean(abs_err1,axis=0)\n",
    "\n",
    "abs_err2 = np.absolute(test_pred2_mvr, spd_mat[30529:30529+3712,])\n",
    "mean_abs_err2 = np.mean(abs_err2,axis=0)\n",
    "\n",
    "abs_err3 = np.absolute(test_pred3_mvr, spd_mat[30529:30529+3712,])\n",
    "mean_abs_err3 = np.mean(abs_err3,axis=0)\n",
    "\n",
    "abs_err4 = np.absolute(test_pred4_mvr, spd_mat[30529:30529+3712,])\n",
    "mean_abs_err4 = np.mean(abs_err4,axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 585,
     "status": "ok",
     "timestamp": 1576343263204,
     "user": {
      "displayName": "Sandeep Mudigonda",
      "photoUrl": "",
      "userId": "02295463398705014066"
     },
     "user_tz": 300
    },
    "id": "arfZ8GUMUndn",
    "outputId": "f8764c9c-5f2e-4754-c91c-968b7fe09688"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(207,)\n"
     ]
    }
   ],
   "source": [
    "print(mean_abs_err0.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 407
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1757,
     "status": "ok",
     "timestamp": 1576344528514,
     "user": {
      "displayName": "Sandeep Mudigonda",
      "photoUrl": "",
      "userId": "02295463398705014066"
     },
     "user_tz": 300
    },
    "id": "05Icyz-NR-st",
    "outputId": "7c3d2d57-dc6c-4f35-fb41-fc56b08dcaa2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAzUAAAF1CAYAAADRK8SpAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<Figure size 1008x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting distribution of error for MAE & RMSE-based models\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot\n",
    "plt.figure()\n",
    "plt.figure(figsize=(14,6))\n",
    "\n",
    "values, base = np.histogram(mean_abs_err0, bins=40)\n",
    "cumulative = np.cumsum(values)\n",
    "plt.plot(base[:-1], cumulative, c='g', label='GRU 2 Layer MAE')\n",
    "\n",
    "values, base = np.histogram(mean_abs_err1, bins=40)\n",
    "cumulative = np.cumsum(values)\n",
    "plt.plot(base[:-1], cumulative, c='c', label='LSTM 1 Layer RMSE')\n",
    "\n",
    "values, base = np.histogram(mean_abs_err2, bins=40)\n",
    "cumulative = np.cumsum(values)\n",
    "plt.plot(base[:-1], cumulative, c='m', label='GRU 1 Layer RMSE')\n",
    "\n",
    "values, base = np.histogram(mean_abs_err3, bins=40)\n",
    "cumulative = np.cumsum(values)\n",
    "plt.plot(base[:-1], cumulative, c='r', label='GRU 3 Layer RMSE')\n",
    "\n",
    "values, base = np.histogram(mean_abs_err4, bins=40)\n",
    "cumulative = np.cumsum(values)\n",
    "plt.plot(base[:-1], cumulative, c='b', label='GRU 2 Layer RMSE')\n",
    "plt.title('Distribution of Prediction Error Comparison')\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "0Kjr--ZTZ3mk",
    "rwda0VVKZzuX",
    "_h_T1-7rZu_z",
    "qpKW6BXp6xEX"
   ],
   "name": "RNN Architectures with RMSE .ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
